{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f828617-1ced-47fe-9a15-418d2dc4f8c2",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724e178d-0aa4-426d-8df8-b2394fbdb50c",
   "metadata": {},
   "source": [
    "### Environment and File Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5b0f0d-beae-4dff-a040-1f77b24cfaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "print(\"Environment check:\")\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "print(\"NumPy:\", np.__version__)\n",
    "import sklearn, imblearn\n",
    "print(\"scikit-learn:\", sklearn.__version__)\n",
    "print(\"imbalanced-learn:\", imblearn.__version__)\n",
    "print(\"XGBoost:\", xgb.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ba1d67-6161-4ed5-a2a1-7f23288561dd",
   "metadata": {},
   "source": [
    "### Load Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace42d0d-fee5-4ebf-a908-afd4e7868577",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "# If user started Jupyter inside notebooks/, move one level up\n",
    "if os.path.basename(cwd) == \"notebooks\":\n",
    "    project_root = os.path.dirname(cwd)\n",
    "else:\n",
    "    project_root = cwd\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "data_dir = os.path.join(project_root, \"data\")\n",
    "processed_dir = os.path.join(data_dir, \"processed\")\n",
    "train_path = os.path.join(processed_dir, \"train.csv\")\n",
    "\n",
    "if not os.path.exists(train_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing {train_path}. \"\n",
    "        \"Make sure you ran 01_preprocessing.ipynb successfully first.\"\n",
    "    )\n",
    "\n",
    "print(f\"Found training data: {train_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ad972-0713-48e2-a546-3bc76034f37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_path)\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecd0367-608d-4393-814d-60b5bcd2a820",
   "metadata": {},
   "source": [
    "### Separate Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310404ea-1752-48ed-b039-dcc0092c1ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"label\" not in train_df.columns:\n",
    "    raise KeyError(\"Expected column 'label' in train.csv (binary insider / normal).\")\n",
    "\n",
    "# Features: drop 'label' and 'insider' (scenario ID) if present\n",
    "drop_cols = [\"label\"]\n",
    "if \"insider\" in train_df.columns:\n",
    "    drop_cols.append(\"insider\")\n",
    "\n",
    "X = train_df.drop(columns=drop_cols)\n",
    "y = train_df[\"label\"].astype(int)\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Label vector shape:\", y.shape)\n",
    "print(\"\\nLabel distribution (train):\")\n",
    "print(y.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985fe708-8e66-46d6-ac8b-48b48af3528f",
   "metadata": {},
   "source": [
    "### Define Models and SMOTE pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c067d1e-a6b7-42d7-8870-fd8ad1c31052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "import xgboost as xgb\n",
    "from collections import Counter\n",
    "\n",
    "def smote_pipeline(model, sampling=0.3, scale=False):\n",
    "    \"\"\"\n",
    "    Create a pipeline with SMOTE + optional scaling for models\n",
    "    that benefit from oversampling (Logistic Regression, SVM).\n",
    "    \"\"\"\n",
    "    steps = [('smote', SMOTE(random_state=42, sampling_strategy=sampling))]\n",
    "    if scale:\n",
    "        steps.append(('scale', StandardScaler()))\n",
    "    steps.append(('model', model))\n",
    "    return ImbPipeline(steps=steps)\n",
    "    \n",
    "# Define Models\n",
    "models = {}\n",
    "\n",
    "# 1) Logistic Regression + SMOTE + scaling\n",
    "models[\"LogReg + SMOTE\"] = smote_pipeline(\n",
    "    LogisticRegression(\n",
    "        solver='lbfgs',\n",
    "        max_iter=500,\n",
    "        tol=1e-3,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    ),\n",
    "    sampling=0.3,\n",
    "    scale=True\n",
    ")\n",
    "\n",
    "# 2) Linear SVM + SMOTE + scaling\n",
    "models[\"Linear SVM + SMOTE\"] = smote_pipeline(\n",
    "    LinearSVC(\n",
    "        class_weight='balanced',\n",
    "        max_iter=5000,\n",
    "        tol=1e-3,\n",
    "        random_state=42\n",
    "    ),\n",
    "    sampling=0.3,\n",
    "    scale=True\n",
    ")\n",
    "\n",
    "# 3) Random Forest (NO SMOTE, uses class_weight)\n",
    "models[\"Random Forest\"] = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 4) XGBoost (NO SMOTE, uses scale_pos_weight)\n",
    "counter = Counter(y) \n",
    "scale_pos_weight = counter[0] / counter[1]\n",
    "print(f\"scale_pos_weight for XGBoost: {scale_pos_weight:.2f}\")\n",
    "\n",
    "models[\"XGBoost\"] = xgb.XGBClassifier(\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60a638e-c6b9-48a1-aca3-fb4411b04fd7",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "This may take a while to complete running as it is running on 5 folds. Don't be afraid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bf77f2-e5d8-4c7f-b7ab-3ad404fa45d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scoring = {\n",
    "    \"precision\": \"precision\",\n",
    "    \"recall\": \"recall\",\n",
    "    \"f1\": \"f1\",\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n***Evaluating: {name}***\")\n",
    "    cv_result = cross_validate(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False\n",
    "    )\n",
    "    \n",
    "    precision_mean = cv_result[\"test_precision\"].mean()\n",
    "    recall_mean    = cv_result[\"test_recall\"].mean()\n",
    "    f1_mean        = cv_result[\"test_f1\"].mean()\n",
    "    \n",
    "    precision_std = cv_result[\"test_precision\"].std()\n",
    "    recall_std    = cv_result[\"test_recall\"].std()\n",
    "    f1_std        = cv_result[\"test_f1\"].std()\n",
    "    \n",
    "    print(f\"Precision: {precision_mean:.4f} ± {precision_std:.4f}\")\n",
    "    print(f\"Recall:    {recall_mean:.4f} ± {recall_std:.4f}\")\n",
    "    print(f\"F1-score:  {f1_mean:.4f} ± {f1_std:.4f}\")\n",
    "    \n",
    "    results.append({\n",
    "        \"model\": name,\n",
    "        \"precision_mean\": precision_mean,\n",
    "        \"precision_std\": precision_std,\n",
    "        \"recall_mean\": recall_mean,\n",
    "        \"recall_std\": recall_std,\n",
    "        \"f1_mean\": f1_mean,\n",
    "        \"f1_std\": f1_std,\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d0543e-ee0b-408c-9a81-b5e7d3b8e921",
   "metadata": {},
   "source": [
    "### Sort results to determine best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6799c7e5-3b6f-460a-a31b-a2d7672c2897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = results_df.copy()\n",
    "\n",
    "df[\"precision_rank\"] = df[\"precision_mean\"].rank(ascending=False).astype(int)\n",
    "df[\"recall_rank\"] = df[\"recall_mean\"].rank(ascending=False).astype(int)\n",
    "df[\"f1_rank\"] = df[\"f1_mean\"].rank(ascending=False).astype(int)\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Model\": df[\"model\"],\n",
    "    \"Precision\": df[\"precision_mean\"].round(3).astype(str),\n",
    "    \"Recall\": df[\"recall_mean\"].round(3).astype(str),\n",
    "    \"F1 Score\": df[\"f1_mean\"].round(3).astype(str),\n",
    "    \"Precision Rank\": df[\"precision_rank\"],\n",
    "    \"Recall Rank\": df[\"recall_rank\"],\n",
    "    \"F1 Rank\": df[\"f1_rank\"],\n",
    "})\n",
    "\n",
    "# Sort by best model (highest F1)\n",
    "summary_sorted = summary.sort_values(by=\"F1 Rank\")\n",
    "\n",
    "summary_sorted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110f510a-71b3-4901-9c2f-2875456b84a4",
   "metadata": {},
   "source": [
    "# Key Observations\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "RF becomes overly conservative, predicting threats only in extremely confident cases.\n",
    "\n",
    "High precision but  poor recall will lead to missing most insider threats\n",
    "\n",
    "### Logistic Regression + SMOTE\n",
    "\n",
    "Produces many false positives\n",
    "\n",
    "High recall comes at the cost of extremely poor precision\n",
    "\n",
    "### Linear SVM + SMOTE\n",
    "\n",
    "More balanced than logistic regression and RF\n",
    "\n",
    "Moderate precision and recall\n",
    "\n",
    "Still underperforms compared to XGBoost\n",
    "\n",
    "### XGBoost\n",
    "\n",
    "Best overall performance\n",
    "\n",
    "Strong precision and recall balance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb4c002-0d84-40dc-ba20-d288ac783afa",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "XGBoost is the strongest baseline model and will make a strong foundation for the next stage of the pipeline.\n",
    "\n",
    "The next notebook (03_modelTuning) will focus on hyperparameter optimization using RandomizedSearchCV to further improve performance before feature selection and threshold calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa64c36b-41dd-4d2c-951b-91b3861d3a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "certml",
   "language": "python",
   "name": "certml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
