{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e23d7beb-f1a8-4688-8e1a-1e4106f0a308",
   "metadata": {},
   "source": [
    "# Model Optimization, Evaluation, and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1ad87d-1074-421f-9b58-131b371b4035",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405a7373-8765-4ba5-a74a-843794de459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, accuracy_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    precision_recall_curve, roc_curve, auc\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Environment check:\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a19fdd-02ae-4f58-8796-1c00705746b6",
   "metadata": {},
   "source": [
    "### Load Reduced Feature Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9283e355-4289-4687-8f8d-0d64826afa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "if os.path.basename(cwd) == \"notebooks\":\n",
    "    project_root = os.path.dirname(cwd)\n",
    "else:\n",
    "    project_root = cwd\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "model_path = os.path.join(project_root, \"models\", \"xgb_reduced_model.joblib\")\n",
    "model = joblib.load(model_path)\n",
    "print(f\"Loaded model: {model_path}\")\n",
    "\n",
    "with open(os.path.join(project_root, \"models\", \"feature_list.json\"), \"r\") as f:\n",
    "    feature_list = json.load(f)\n",
    "print(f\"Loaded {len(feature_list)} features\")\n",
    "\n",
    "val_path = os.path.join(project_root, \"data\", \"processed\", \"val.csv\")\n",
    "test_path = os.path.join(project_root, \"data\", \"processed\", \"test.csv\")\n",
    "\n",
    "df_val = pd.read_csv(val_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "\n",
    "y_val = df_val[\"label\"]\n",
    "X_val = df_val[feature_list]\n",
    "\n",
    "y_test = df_test[\"label\"]\n",
    "X_test = df_test[feature_list]\n",
    "\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nValidation positives: {y_val.sum()}\")\n",
    "print(f\"Test positives: {y_test.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2299bbec-e820-40cd-8f03-9064e0a2c211",
   "metadata": {},
   "source": [
    "### Probability Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8e7161-15d6-4652-a6b0-67720225e00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_probs = model.predict_proba(X_val)[:, 1]\n",
    "test_probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Validation probability statistics:\")\n",
    "print(f\"  Min:    {val_probs.min():.4f}\")\n",
    "print(f\"  Max:    {val_probs.max():.4f}\")\n",
    "print(f\"  Mean:   {val_probs.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(val_probs):.4f}\")\n",
    "\n",
    "print(\"\\nTest probability statistics:\")\n",
    "print(f\"  Min:    {test_probs.min():.4f}\")\n",
    "print(f\"  Max:    {test_probs.max():.4f}\")\n",
    "print(f\"  Mean:   {test_probs.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(test_probs):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b111b40-8de2-42e9-895f-2c5e3bcd58f3",
   "metadata": {},
   "source": [
    "### Plot Calibration curve on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5a96c9-ebdd-4253-bcf3-7bc5e070136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "figures_dir = os.path.join(project_root, \"figures\")\n",
    "os.makedirs(figures_dir, exist_ok=True)\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(y_val, val_probs, n_bins=10)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(prob_pred, prob_true, marker='o', linewidth=2, markersize=8, label='XGBoost')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect Calibration')\n",
    "plt.xlabel('Mean Predicted Probability', fontsize=12)\n",
    "plt.ylabel('Fraction of Positives', fontsize=12)\n",
    "plt.title('Calibration Curve - Validation Set', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(figures_dir, 'calibration_curve.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Calibration curve saved to figures/calibration_curve.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c9464a-0ae1-4a27-ac8d-c7f9fb41ccbf",
   "metadata": {},
   "source": [
    "### Threshold Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ffb515-c053-4653-9cef-c43e51cddf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 500)\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "\n",
    "for t in thresholds:\n",
    "    preds = (val_probs >= t).astype(int)\n",
    "    precision_list.append(precision_score(y_val, preds, zero_division=0))\n",
    "    recall_list.append(recall_score(y_val, preds, zero_division=0))\n",
    "    f1_list.append(f1_score(y_val, preds, zero_division=0))\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"threshold\": thresholds,\n",
    "    \"precision\": precision_list,\n",
    "    \"recall\": recall_list,\n",
    "    \"f1\": f1_list\n",
    "})\n",
    "\n",
    "print(\"Threshold sweep complete.\")\n",
    "print(f\"\\nThresholds evaluated: {len(thresholds)}\")\n",
    "print(f\"Best F1 score: {results_df['f1'].max():.4f} at threshold {results_df.loc[results_df['f1'].idxmax(), 'threshold']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960cd3a6-8629-4a3c-be6c-271a585d8776",
   "metadata": {},
   "source": [
    "### Threshold Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2929bca3-84b1-499b-9459-fe5ac1c034c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_candidates = results_df[results_df['recall'] >= 0.75]\n",
    "if len(alert_candidates) > 0:\n",
    "    alert_threshold = alert_candidates.loc[alert_candidates['f1'].idxmax(), 'threshold']\n",
    "    alert_metrics = alert_candidates.loc[alert_candidates['f1'].idxmax()]\n",
    "else:\n",
    "    alert_threshold = results_df.loc[results_df['recall'].idxmax(), 'threshold']\n",
    "    alert_metrics = results_df.loc[results_df['recall'].idxmax()]\n",
    "\n",
    "critical_candidates = results_df[results_df['precision'] >= 0.90]\n",
    "if len(critical_candidates) > 0:\n",
    "    critical_threshold = critical_candidates.loc[critical_candidates['f1'].idxmax(), 'threshold']\n",
    "    critical_metrics = critical_candidates.loc[critical_candidates['f1'].idxmax()]\n",
    "else:\n",
    "    critical_threshold = results_df.loc[results_df['precision'].idxmax(), 'threshold']\n",
    "    critical_metrics = results_df.loc[results_df['precision'].idxmax()]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SELECTED THRESHOLDS (Validation Set)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nAlert Mode Threshold: {alert_threshold:.3f}\")\n",
    "print(f\"  Precision: {alert_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:    {alert_metrics['recall']:.4f}\")\n",
    "print(f\"  F1-Score:  {alert_metrics['f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nCritical Mode Threshold: {critical_threshold:.3f}\")\n",
    "print(f\"  Precision: {critical_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:    {critical_metrics['recall']:.4f}\")\n",
    "print(f\"  F1-Score:  {critical_metrics['f1']:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f6bf6f-5978-4b99-be72-a782ff733e55",
   "metadata": {},
   "source": [
    "### Visualize Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85efc30-c518-438d-91d1-a0e7116e4ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(results_df['threshold'], results_df['precision'], label='Precision', linewidth=2.5, alpha=0.8)\n",
    "plt.plot(results_df['threshold'], results_df['recall'], label='Recall', linewidth=2.5, alpha=0.8)\n",
    "plt.plot(results_df['threshold'], results_df['f1'], label='F1-Score', linewidth=2.5, alpha=0.8)\n",
    "\n",
    "plt.axvline(x=alert_threshold, color='green', linestyle='--', linewidth=2, \n",
    "            label=f'Alert Mode ({alert_threshold:.2f})', alpha=0.7)\n",
    "plt.axvline(x=critical_threshold, color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Critical Mode ({critical_threshold:.2f})', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Classification Threshold', fontsize=12)\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.title('Performance Metrics vs Classification Threshold', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11, loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(figures_dir, 'threshold_analysis.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Threshold analysis plot saved to figures/threshold_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b00c11-e99a-4214-a788-c7e9c73e0bd4",
   "metadata": {},
   "source": [
    "# Our Model is validated, calibrated, and optimized! It is time to test!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b70c39-7fdf-475a-963c-3102fe065f4f",
   "metadata": {},
   "source": [
    "### Alert Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdf89b4-e447-4cb7-a968-2cb1a621c366",
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_preds = (test_probs >= alert_threshold).astype(int)\n",
    "\n",
    "alert_precision = precision_score(y_test, alert_preds)\n",
    "alert_recall = recall_score(y_test, alert_preds)\n",
    "alert_f1 = f1_score(y_test, alert_preds)\n",
    "alert_accuracy = accuracy_score(y_test, alert_preds)\n",
    "alert_cm = confusion_matrix(y_test, alert_preds)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"ALERT MODE - TEST SET (Threshold = {alert_threshold:.2f})\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Precision: {alert_precision:.4f}\")\n",
    "print(f\"Recall:    {alert_recall:.4f}\")\n",
    "print(f\"F1-Score:  {alert_f1:.4f}\")\n",
    "print(f\"Accuracy:  {alert_accuracy:.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"                Predicted\")\n",
    "print(f\"                Normal  Threat\")\n",
    "print(f\"Actual Normal   {alert_cm[0,0]:6d}  {alert_cm[0,1]:6d}\")\n",
    "print(f\"       Threat   {alert_cm[1,0]:6d}  {alert_cm[1,1]:6d}\")\n",
    "print(f\"\\nFalse Positives: {alert_cm[0,1]} ({alert_cm[0,1]/alert_cm[0].sum()*100:.2f}% of normals)\")\n",
    "print(f\"False Negatives: {alert_cm[1,0]} ({alert_cm[1,0]/alert_cm[1].sum()*100:.2f}% of threats)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc611df-01d0-44d0-b237-88c40f1518cb",
   "metadata": {},
   "source": [
    "### Test Critical Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f1a907-90fb-4341-99ac-15cea708b036",
   "metadata": {},
   "outputs": [],
   "source": [
    "critical_preds = (test_probs >= critical_threshold).astype(int)\n",
    "\n",
    "critical_precision = precision_score(y_test, critical_preds)\n",
    "critical_recall = recall_score(y_test, critical_preds)\n",
    "critical_f1 = f1_score(y_test, critical_preds)\n",
    "critical_accuracy = accuracy_score(y_test, critical_preds)\n",
    "critical_cm = confusion_matrix(y_test, critical_preds)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"CRITICAL MODE - TEST SET (Threshold = {critical_threshold:.2f})\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Precision: {critical_precision:.4f}\")\n",
    "print(f\"Recall:    {critical_recall:.4f}\")\n",
    "print(f\"F1-Score:  {critical_f1:.4f}\")\n",
    "print(f\"Accuracy:  {critical_accuracy:.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"                Predicted\")\n",
    "print(f\"                Normal  Threat\")\n",
    "print(f\"Actual Normal   {critical_cm[0,0]:6d}  {critical_cm[0,1]:6d}\")\n",
    "print(f\"       Threat   {critical_cm[1,0]:6d}  {critical_cm[1,1]:6d}\")\n",
    "print(f\"\\nFalse Positives: {critical_cm[0,1]} ({critical_cm[0,1]/critical_cm[0].sum()*100:.2f}% of normals)\")\n",
    "print(f\"False Negatives: {critical_cm[1,0]} ({critical_cm[1,0]/critical_cm[1].sum()*100:.2f}% of threats)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a980f3-25a0-4785-860f-a1c405f733e4",
   "metadata": {},
   "source": [
    "# Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd00b26-20c4-41ac-a924-395481c0a8ad",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2b68f8-e8f6-40f2-bc6b-1705816e68f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Alert Mode\n",
    "sns.heatmap(alert_cm, annot=True, fmt='d', cmap='Blues', ax=axes[0], \n",
    "            xticklabels=['Normal', 'Threat'], yticklabels=['Normal', 'Threat'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "axes[0].set_title(f'Alert Mode (Threshold={alert_threshold:.2f})\\nF1={alert_f1:.3f}', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Actual', fontsize=11)\n",
    "axes[0].set_xlabel('Predicted', fontsize=11)\n",
    "\n",
    "# Critical Mode\n",
    "sns.heatmap(critical_cm, annot=True, fmt='d', cmap='Reds', ax=axes[1],\n",
    "            xticklabels=['Normal', 'Threat'], yticklabels=['Normal', 'Threat'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "axes[1].set_title(f'Critical Mode (Threshold={critical_threshold:.2f})\\nF1={critical_f1:.3f}', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Actual', fontsize=11)\n",
    "axes[1].set_xlabel('Predicted', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(figures_dir, 'confusion_matrices.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion matrices saved to figures/confusion_matrices.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961903b2-4e9c-4bb6-a76d-7ea0f79f98e5",
   "metadata": {},
   "source": [
    "### ROC and Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b73a922-be5a-4eb9-a537-bc207c6fdb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_test, test_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(y_test, test_probs)\n",
    "pr_auc = auc(recall_curve, precision_curve)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC Curve\n",
    "axes[0].plot(fpr, tpr, color='darkorange', lw=2.5, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "axes[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize=11)\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize=11)\n",
    "axes[0].set_title('ROC Curve - Test Set', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "axes[1].plot(recall_curve, precision_curve, color='blue', lw=2.5, label=f'PR curve (AUC = {pr_auc:.3f})')\n",
    "axes[1].scatter([alert_recall], [alert_precision], color='green', s=150, zorder=5, \n",
    "                marker='o', edgecolors='black', linewidths=2, label=f'Alert Mode')\n",
    "axes[1].scatter([critical_recall], [critical_precision], color='red', s=150, zorder=5,\n",
    "                marker='s', edgecolors='black', linewidths=2, label=f'Critical Mode')\n",
    "axes[1].set_xlabel('Recall', fontsize=11)\n",
    "axes[1].set_ylabel('Precision', fontsize=11)\n",
    "axes[1].set_title('Precision-Recall Curve - Test Set', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(figures_dir, 'roc_pr_curves.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ROC and PR curves saved to figures/roc_pr_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b7d2f5-3cd6-4a48-962a-b7c47a2e96e2",
   "metadata": {},
   "source": [
    "### Error Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bfbe8b-413c-4cb7-824a-45d2ed2ef79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Alert Mode errors\n",
    "print(\"=\" * 60)\n",
    "print(\"ERROR ANALYSIS - ALERT MODE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# False Positives\n",
    "fp_indices = ((alert_preds == 1) & (y_test == 0))\n",
    "fp_probs = test_probs[fp_indices]\n",
    "\n",
    "print(f\"\\nFalse Positives: {fp_indices.sum()}\")\n",
    "if len(fp_probs) > 0:\n",
    "    print(f\"  Probability range: [{fp_probs.min():.3f}, {fp_probs.max():.3f}]\")\n",
    "    print(f\"  Probability mean:  {fp_probs.mean():.3f}\")\n",
    "    print(f\"  Probability std:   {fp_probs.std():.3f}\")\n",
    "\n",
    "# False Negatives\n",
    "fn_indices = ((alert_preds == 0) & (y_test == 1))\n",
    "fn_probs = test_probs[fn_indices]\n",
    "\n",
    "print(f\"\\nFalse Negatives: {fn_indices.sum()}\")\n",
    "if len(fn_probs) > 0:\n",
    "    print(f\"  Probability range: [{fn_probs.min():.3f}, {fn_probs.max():.3f}]\")\n",
    "    print(f\"  Probability mean:  {fn_probs.mean():.3f}\")\n",
    "    print(f\"  Probability std:   {fn_probs.std():.3f}\")\n",
    "\n",
    "# True Positives\n",
    "tp_indices = ((alert_preds == 1) & (y_test == 1))\n",
    "tp_probs = test_probs[tp_indices]\n",
    "\n",
    "print(f\"\\nTrue Positives: {tp_indices.sum()}\")\n",
    "if len(tp_probs) > 0:\n",
    "    print(f\"  Probability range: [{tp_probs.min():.3f}, {tp_probs.max():.3f}]\")\n",
    "    print(f\"  Probability mean:  {tp_probs.mean():.3f}\")\n",
    "    print(f\"  Probability std:   {tp_probs.std():.3f}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b0f942-6eb0-4d56-8235-3c2e4b3ea2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize probability distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(test_probs[y_test == 0], bins=50, alpha=0.6, label='Normal', color='blue')\n",
    "axes[0].hist(test_probs[y_test == 1], bins=50, alpha=0.6, label='Threat', color='red')\n",
    "axes[0].axvline(x=alert_threshold, color='green', linestyle='--', linewidth=2, label='Alert')\n",
    "axes[0].axvline(x=critical_threshold, color='darkred', linestyle='--', linewidth=2, label='Critical')\n",
    "axes[0].set_xlabel('Predicted Probability', fontsize=11)\n",
    "axes[0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[0].set_title('Probability Distribution by Class', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "box_data = [test_probs[y_test == 0], test_probs[y_test == 1]]\n",
    "bp = axes[1].boxplot(box_data, tick_labels=['Normal', 'Threat'], patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('lightblue')\n",
    "bp['boxes'][1].set_facecolor('lightcoral')\n",
    "axes[1].axhline(y=alert_threshold, color='green', linestyle='--', linewidth=2, label='Alert')\n",
    "axes[1].axhline(y=critical_threshold, color='darkred', linestyle='--', linewidth=2, label='Critical')\n",
    "axes[1].set_ylabel('Predicted Probability', fontsize=11)\n",
    "axes[1].set_title('Probability Distribution Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(figures_dir, 'probability_distributions.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Probability distributions saved to figures/probability_distributions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bfedcc-822a-4a81-a3ae-6e525f8fc19c",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e31d72-49f8-47b2-b97f-12ac272be78f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook completed comprehensive evaluation of the final XGBoost model through calibration analysis, threshold optimization, and performance visualization on the held-out test set.\n",
    "\n",
    "### Calibration Curve\n",
    "The calibration curve demonstrates good model reliability, with predicted probabilities closely tracking actual threat rates.\n",
    "\n",
    "### Threshold Analysis\n",
    "The performance-vs-threshold plot reveals clear precision-recall trade-offs across the full probability spectrum. \n",
    "\n",
    "### Confusion Matrices\n",
    "Side-by-side confusion matrices illustrate the operational differences between modes:\n",
    "- Alert Mode: Demonstrates optimal balanced detection\n",
    "- Critical Mode: Dramatic precision advantage with recall trade-off\n",
    "\n",
    "### ROC and Precision-Recall Curves\n",
    "The ROC curve (AUC=0.997) hugs the top-left corner, demonstrating near-perfect class discrimination. The PR curve (AUC=0.840) maintains high precision across a wide recall range, with both operating points well placed. Critical Mode on the precision plateau, Alert Mode at the precision-recall balance point. These curves validate model performance despite extreme class imbalance.\n",
    "\n",
    "### Probability Distributions\n",
    "The histogram reveals ideal bimodal separation. The box plot confirms minimal overlap, with threat median matching the Critical Mode threshold. This clear class separation validates strong feature selection and optimal threshold placement.\n",
    "\n",
    "## Final Model Performance\n",
    "\n",
    "**Alert Mode (Threshold = 0.26):**\n",
    "- Precision: 77.78% | Recall: 77.78% | F1: 0.7778\n",
    "  \n",
    "**Critical Mode (Threshold = 0.65):**\n",
    "- Precision: 97.78% | Recall: 69.84% | F1: 0.8148"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d531df1-ee49-4840-82b0-70f3d366fb44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "certml",
   "language": "python",
   "name": "certml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
